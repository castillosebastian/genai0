{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Splitter, Chunk-Size y Overlap**\n",
    "\n",
    "   > ´RecursiveCharacterTextSplitter(chunk_size=1000/2000, chunk_overlap=100/200)´\n",
    "\n",
    "   Fundamento:\n",
    "   1) Results of experiments conducted on Azure Cognitive Search presents quantitative basis to support chunk size and overlap around those values, in the Generative AI scenarios where applications use the RAG pattern. [link](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167).\n",
    "   2) Azure Demo Archictecture (Backend-Frontend) of a RAG systems: ingest [script](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/scripts/prepdocslib/textsplitter.py). \n",
    "   3) Personal Intuition: GPT-3.5-turbo supports a context window of 4096 tokens (8192 for gpt-4): that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error. So we 100% need to keep below this (see [Openai Managing Tokens](https://platform.openai.com/docs/guides/text-generation/managing-tokens)). If we assume a very safe margin of ~2000 tokens for the input prompt into gpt-3.5-turbo, leaving ~2000 tokens for conversation history and completion. With this ~2000 token limit we can include: \n",
    "      - 5 snippets of relevant information, meaning each snippet can be no more than 400 token long (or 2000 characters), or\n",
    "      - 4 x 500 (2500 characters). \n",
    "   4) FinanceBench used entirely pages as context/chunk. One page context size reference:\n",
    "      1) 1 page 3M-10k-2022, BalanceSheet = 285 tokens / 2000 characters \n",
    "      2) 1 page 3M-10k-2022, MD&A = 707 tokens / 4650 characters     \n",
    "      Posible issue 'lost in the middle'! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Metada**\n",
    "\n",
    "    Minimal: \n",
    "\n",
    "        > doc_type: `[UseCase#1-#4]_DataSource``\n",
    "        > \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
