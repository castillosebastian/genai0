{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Splitter, Chunk-Size y Overlap**\n",
    "\n",
    "   > ´RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)´\n",
    "\n",
    "   Arguments:\n",
    "   \n",
    "   1) Results of experiments conducted on Azure Cognitive Search presents quantitative basis to support chunk size and overlap around those values, in the Generative AI scenarios where applications use the RAG pattern. [link](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-cognitive-search-outperforming-vector-search-with-hybrid/ba-p/3929167).\n",
    "   2) Result of single experiment 1024 chunk-size by [Llama-index](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\n",
    "   3) Azure Demo Archictecture (Backend-Frontend) of a RAG systems: ingest [script](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/scripts/prepdocslib/textsplitter.py). \n",
    "   4) Personal Intuition: GPT-3.5-turbo supports a context window of 4096 tokens (8192 for gpt-4): that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error. So we 100% need to keep below this (see [Openai Managing Tokens](https://platform.openai.com/docs/guides/text-generation/managing-tokens)). If we assume a very safe margin of ~2000 tokens for the input prompt into gpt-3.5-turbo, leaving ~2000 tokens for conversation history and completion. With this ~2000 token limit we can include: \n",
    "      - 5 snippets of relevant information, meaning each snippet can be no more than 400 token long (or 2000 characters), or\n",
    "      - 4 x 500 (2500 characters). \n",
    "   5) FinanceBench used entirely pages as context/chunk. One page context size reference:\n",
    "      1) 1 page 3M-10k-2022, BalanceSheet = 285 tokens / 2000 characters \n",
    "      2) 1 page 3M-10k-2022, MD&A = 707 tokens / 4650 characters     \n",
    "      Posible issue 'lost in the middle'! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Metada**\n",
    "\n",
    "Metada refers to qualities related to the document and qualities related to the entity (organization) described by de document.\n",
    "\n",
    "\n",
    "From EY: \n",
    "\n",
    "> doc_type: `[UseCase#1-#4]_DataSource``   \n",
    "\n",
    "> doc_url: `[UseCase#1-#4]_Location``\n",
    "    \n",
    "From FinanceBench:\n",
    "\n",
    "> id: A unique ID.\n",
    "\n",
    "> doc_author: Author of the document.\n",
    "\n",
    "> company_name: The company’s name. \n",
    "\n",
    "> company_sector: The company’s sector following GICS sector definitions. \n",
    "\n",
    "> doc_filing_name: The name of the public filing used to pose and answer the question. \n",
    "\n",
    "> doc_filing_label: label for the document type (ej.10-K)\n",
    "\n",
    "> filing_url: A link to the relevant public filing. Where possible, from the company’s investor, EDGAR.\n",
    "\n",
    "> doc_year: The fiscal year that the document is referencing.\n",
    "\n",
    "From UseCases:\n",
    "\n",
    "> company_country\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
