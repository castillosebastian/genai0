{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset FinanceBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import time\n",
    "\n",
    "import sentence_transformers\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load OpenAI access\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../src'))\n",
    "from azure_openai_conn import OpenAIembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn huggingface dataset to pd\n",
    "# images = fashion[\"image\"]\n",
    "# data = fashion.remove_columns(\"image\")\n",
    "# product_df = data.to_pandas()\n",
    "# product_data = product_df.reset_index(drop=True).to_dict(orient=\"index\")\n",
    "if os.path.isfile('../../data/financebench_sample_150.csv'):\n",
    "    df = pd.read_csv('../../data/financebench_sample_150.csv')\n",
    "else:    \n",
    "    ds = load_dataset(\"PatronusAI/financebench\")\n",
    "    df = pd.DataFrame(ds)\n",
    "    all_dicts = []\n",
    "    for index, row in df.iterrows():    \n",
    "        dictionary = row['train']    \n",
    "        all_dicts.append(dictionary)\n",
    "    df = pd.DataFrame(all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "destination_folder = '../../data/financebench'\n",
    "\n",
    "if not os.path.exists(destination_folder):\n",
    "\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['doc_link']\n",
    "        doc_name = row['doc_name']\n",
    "        doc_name_with_extension = doc_name + '.pdf'        \n",
    "        file_path = os.path.join(destination_folder, doc_name_with_extension)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:            \n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded: {doc_name_with_extension}\")\n",
    "        else:\n",
    "            print(f\"Failed to download: {doc_name_with_extension} ({url})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCACOLA_2021_10K.pdf\n",
      "PFIZER_2021_10K.pdf\n",
      "VERIZON_2022_10K.pdf\n",
      "PEPSICO_2021_10K.pdf\n",
      "NETFLIX_2017_10K.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_folder_path = destination_folder\n",
    "documents = []\n",
    "for file in os.listdir(pdf_folder_path)[:5]:\n",
    "    print(file)\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder_path, file)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition about chunk-size: \n",
    "GPT-3.5-turbo supports a context window of 4096 tokens â€” that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error. So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into gpt-3.5-turbo, leaving ~2000 tokens for conversation history and completion. With this ~2000 token limit we can include \n",
    "- 5 snippets of relevant information, meaning each snippet can be no more than 400 token long, or\n",
    "- 4 x 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIembeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some grid-search\n",
    "pchunks_list = [400, 600, 800, 1000]\n",
    "poverlap_list = [30, 50, 80]\n",
    "\n",
    "# Loop over each combination of chunk_size and chunk_overlap\n",
    "for chunk_size in pchunks_list:\n",
    "    for chunk_overlap in poverlap_list:\n",
    "        try:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True)\n",
    "            chunked_documents = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Create a unique directory name for each combination\n",
    "            persist_directory = f'chroma_{chunk_size}_{chunk_overlap}'\n",
    "            \n",
    "            chroma = Chroma.from_documents(documents=chunked_documents, embedding=embeddings, persist_directory=persist_directory)\n",
    "\n",
    "            # Additional code to process or store the results can be added here\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with chunk_size={chunk_size} and chunk_overlap={chunk_overlap}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genai0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
