{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/castillosebastian/genai0/blob/main/exp/Ragbot_template.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment Plan\n",
        "\n",
        "Evaluate results on accuracy and speed.\n",
        "Comparison:\n",
        "- Basic Tables retriever.\n",
        "- Advance Table retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGbot Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target doc to chat with\n",
        "url_doc_to_chat = \"https://ir.tesla.com/_flysystem/s3/sec/000095017023001409/tsla-20221231-gen.pdf\"\n",
        "# Build VDB with Chroma\n",
        "chunk_size = 1024\n",
        "chunk_overlap = 100\n",
        "k_docs_context = 3\n",
        "embeddings_model = \"thenlper/gte-large\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set-up LLM Mistral 7b-Q4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyP9j5V3Py9f",
        "outputId": "62313073-b9eb-4786-88f9-c6feacd934fc"
      },
      "outputs": [],
      "source": [
        "# Inspired by, Nour Eddine Zekaoui, in his post 'Your Web Pages Using Mistral-7b & LangChain', \n",
        "# [github](https://github.com/zekaouinoureddine/Adding-Private-Data-to-LLMs/tree/master)\n",
        "\n",
        "!pip install gradio --quiet\n",
        "!pip install xformer --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install pypdfium2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mistral-7b LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnPZtH1IRNRZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# The following code sets up a text generation pipeline using a base LLM, Mistral-7b \n",
        "# developed by Mistral AI. It instructs a pre-trained language model, configures it with \n",
        "# quantization settings, tokenization, and generation parameters, and creates a pipeline that \n",
        "# can be used for generating text based on the Mistral-7b LLM and configurations. \n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# The star!\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline, #HuggingFacePipeline is a class that allows you to run Hugging Face models locally\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retriever with Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Any\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.vectorstores import DocArrayInMemorySearch, Chroma\n",
        "import uuid\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Ref\n",
        "# intall tesseract!\n",
        "# https://python.langchain.com/docs/integrations/providers/unstructured\n",
        "# https://github.com/Unstructured-IO/unstructured\n",
        "\n",
        "# Process PDF----------------------------------------------------------------------------\n",
        "# See: https://unstructured-io.github.io/unstructured/core/partition.html#partition-pdf\n",
        "start_time_partitionpdf = time.perf_counter()\n",
        "raw_pdf_elements = partition_pdf(    \n",
        "    filename= '/home/sebacastillo/genai0/bd/Apple_2023.pdf',\n",
        "    # Unstructured first finds embedded image blocks\n",
        "    extract_images_in_pdf=False,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path='bd/image',\n",
        ")\n",
        "end_time_partitionpdf = time.perf_counter()\n",
        "duration_partition_pdf = end_time_partitionpdf - start_time_partitionpdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57\n",
            "125\n"
          ]
        }
      ],
      "source": [
        "# collect element by type\n",
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    text: Any\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type='table' text='2023 Adjusted Cost Unrealized Gains Unrealized Losses Fair Value Cash and Cash Equivalents Current Marketable Securities Cash $ 28,359 $ — $ — $ 28,359 $ 28,359 $ — $ Level 1: Money market funds Mutual funds and equity securities 481 442 — 12 — (26) 481 428 481 — — 428 Subtotal (1) Level 2 : 923 12 (26) 909 481 428 U.S. Treasury securities U.S. agency securities Non-U.S. government securities Certificates of deposit and time deposits Commercial paper Corporate debt securities Municipal securities Mortgage- and asset-backed securities 19,406 5,736 17,533 1,354 608 76,840 628 22,365 — — 6 — — 6 — 6 (1,292) (600) (1,048) — — (5,956) (26) (2,735) 18,114 5,136 16,491 1,354 608 70,890 602 19,636 35 36 — 1,034 — 20 — — 5,468 271 11,332 320 608 12,627 192 344 Subtotal 144,470 18 (11,657) 132,831 1,125 31,162 Total (2) $ 173,752 $ 30 $ (11,683) $ 162,099 $ 29,965 $ 31,590 $ Non-Current Marketable Securities — — — — 12,611 4,829 5,159 — — 58,243 410 19,292 100,544 100,544'\n"
          ]
        }
      ],
      "source": [
        "print(table_elements[20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGbot SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRfwX1ROffVu"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embeddings_model,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh292g_caV-9",
        "outputId": "44635df6-5ab4-42f2-f652-d84aa0edaa40"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "loader = PyPDFium2Loader(url_doc_to_chat)\n",
        "docs = loader.load()\n",
        "\n",
        "# Vector DB\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "texts_chunks = text_splitter.split_documents(docs)\n",
        "db = Chroma.from_documents(texts_chunks, embeddings, persist_directory=\"db\")\n",
        "\n",
        "# Prompt\n",
        "custom_template = \"\"\"You are a Financial AI Assistant. Given the\n",
        "following conversation and a follow up question, rephrase the follow up question\n",
        "to be a standalone question. At the end of standalone question add this\n",
        "'Answer the question.' If you do not know the answer reply with 'I do not have enough information'.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\n",
        "\"\"\"\n",
        "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n",
        "\n",
        "# R-AG function\n",
        "\n",
        "def querying(query, history):\n",
        "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=db.as_retriever(search_kwargs={\"k\": k_docs_context}), # Top n doc of db\n",
        "      memory=memory,\n",
        "      condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
        ")\n",
        "\n",
        "  result = qa_chain({\"question\": query})\n",
        "  \n",
        "  return result[\"answer\"].strip()\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    fn = querying,\n",
        "    chatbot=gr.Chatbot(height=600),\n",
        "    textbox=gr.Textbox(placeholder=\"Message\", container=False, scale=7),\n",
        "    title=\"POC: RAGgbot\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"What is the Tesla revenue in 2022\",\n",
        "              \"Summarize the balance sheet of Tesla\"],\n",
        "\n",
        "    cache_examples=True,\n",
        "    retry_btn=\"Retry\",\n",
        "    undo_btn=\"Undo\",\n",
        "    clear_btn=\"Clear\",\n",
        "    submit_btn=\"Submit\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Launch app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8gJEQIOiaST"
      },
      "outputs": [],
      "source": [
        "iface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
