{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/castillosebastian/genai0/blob/main/exp/exp2_rag_mistral/exp.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add financial document for the RAGbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_ulr = \"https://ir.tesla.com/_flysystem/s3/sec/000095017023001409/tsla-20221231-gen.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set-up LLM Mistral 7b-Q4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyP9j5V3Py9f",
        "outputId": "62313073-b9eb-4786-88f9-c6feacd934fc"
      },
      "outputs": [],
      "source": [
        "# Inspired by, Nour Eddine Zekaoui, in his post 'Your Web Pages Using Mistral-7b & LangChain', \n",
        "# [github](https://github.com/zekaouinoureddine/Adding-Private-Data-to-LLMs/tree/master)\n",
        "\n",
        "!pip install gradio --quiet\n",
        "!pip install xformer --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install pypdfium2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mistral-7b LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnPZtH1IRNRZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# The following code sets up a text generation pipeline using a base LLM, Mistral-7b \n",
        "# developed by Mistral AI. It instructs a pre-trained language model, configures it with \n",
        "# quantization settings, tokenization, and generation parameters, and creates a pipeline that \n",
        "# can be used for generating text based on the Mistral-7b LLM and configurations. \n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# The star!\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline, #HuggingFacePipeline is a class that allows you to run Hugging Face models locally\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FinSight RAGbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRfwX1ROffVu"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-large\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh292g_caV-9",
        "outputId": "44635df6-5ab4-42f2-f652-d84aa0edaa40"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFium2Loader\n",
        "loader = PyPDFium2Loader(pdf_ulr)\n",
        "docs = loader.load()\n",
        "#len(docs)\n",
        "# docs[0].metadata\n",
        "\n",
        "# Vector DB\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts_chunks = text_splitter.split_documents(docs)\n",
        "db = Chroma.from_documents(texts_chunks, embeddings, persist_directory=\"db\")\n",
        "\n",
        "\n",
        "# Prompt\n",
        "custom_template = \"\"\"You are a Financial AI Assistant. Given the\n",
        "following conversation and a follow up question, rephrase the follow up question\n",
        "to be a standalone question. At the end of standalone question add this\n",
        "'Answer the question in English language.' If you do not know the answer reply with 'I am sorry, I do not have enough information'.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\n",
        "\"\"\"\n",
        "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n",
        "\n",
        "# R-AG function\n",
        "\n",
        "def querying(query, history):\n",
        "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=db.as_retriever(search_kwargs={\"k\": 3}), # Top n doc of db\n",
        "      memory=memory,\n",
        "      condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
        ")\n",
        "\n",
        "  result = qa_chain({\"question\": query})\n",
        "  \n",
        "  return result[\"answer\"].strip()\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    fn = querying,\n",
        "    chatbot=gr.Chatbot(height=600),\n",
        "    textbox=gr.Textbox(placeholder=\"Message FinSight\", container=False, scale=7),\n",
        "    title=\"FinSight Ragbot\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"What is the Tesla revenue in 2022\",\n",
        "              \"Summarize the balance sheet of Tesla\"],\n",
        "\n",
        "    cache_examples=True,\n",
        "    retry_btn=\"Retry\",\n",
        "    undo_btn=\"Undo\",\n",
        "    clear_btn=\"Clear\",\n",
        "    submit_btn=\"Submit\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8gJEQIOiaST"
      },
      "outputs": [],
      "source": [
        "iface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
